{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tugas_DS_CNB.ipynb",
      "provenance": [],
      "mount_file_id": "1Gjd_7phhRiCNPxWcQw26GzQSszWjOF8h",
      "authorship_tag": "ABX9TyPt6K6Bq5x8o/lJUF7w5B29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/permanaaditya/datascience/blob/main/Tugas_DS_CNB_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpJtk4SaD7x0"
      },
      "source": [
        "Spam Classification using Compliment Naives Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TIRsOWYGWbq",
        "outputId": "ff795ad8-2d31-4de6-b4ee-80a3d2e22cbf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEK_qY3KDy_R",
        "outputId": "af8344ca-6331-4e40-d762-fa30205ca63f"
      },
      "source": [
        "import json\n",
        "import numpy as np \n",
        "import pandas as panda\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ============ LOAD DATASET ==============\n",
        "df = panda.read_json('/content/drive/MyDrive/email-text-data.json')\n",
        "\n",
        "# ============ PREPROCESSING =============\n",
        "df.drop_duplicates(inplace = True)\n",
        "#Show the new shape (number of rows & columns)\n",
        "df.shape\n",
        "#Show the number of missing (NAN, NaN, na) data for each column\n",
        "df.isnull().sum()\n",
        "\n",
        "#Need to download stopwords\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "#Tokenization (a list of tokens), will be used as the analyzer\n",
        "#1.Punctuations are [!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\n",
        "#2.Stop words in natural language processing, are useless words (data).\n",
        "def process_text(text):\n",
        "    \n",
        "    #1 Remove Punctuation\n",
        "    nopunc = [char for char in text if char not in string.punctuation]\n",
        "    nopunc = ''.join(nopunc)\n",
        "    \n",
        "    #2 Remove Stop Words\n",
        "    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    #3 Return a list of clean words\n",
        "    return clean_words\n",
        "\n",
        "#Show the Tokenization (a list of tokens )\n",
        "df['MESSAGE'].head().apply(process_text)\n",
        "#df.to_csv('C:/Users/user/new_dataset.csv')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [Dear, Homeowner, Interest, Rates, lowest, poi...\n",
              "1    [ATTENTION, MUST, Computer, Users, NEWSpecial,...\n",
              "2    [multipart, message, MIME, format, NextPart000...\n",
              "3    [IMPORTANT, INFORMATION, new, domain, names, f...\n",
              "4    [bottom, line, GIVE, AWAY, CDs, FREE, people, ...\n",
              "Name: MESSAGE, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8hQMGPkRMtk",
        "outputId": "362647c9-10fb-40a0-e51b-09bfd6a9b8be"
      },
      "source": [
        "messages_bow = CountVectorizer(analyzer=process_text).fit_transform(df['MESSAGE'])\n",
        "#Split data into 80% training & 20% testing data sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(messages_bow, df['CATEGORY'], test_size = 0.20, random_state = 0)\n",
        "#Get the shape of messages_bow\n",
        "messages_bow.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5796, 137833)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQhsWIH0UlGA",
        "outputId": "1d39060c-f271-49d7-ca80-de9651883dfa"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "# Creating and training the Complement Naive Bayes Classifier\n",
        "classifier = ComplementNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "# Evaluating the classifier\n",
        "prediction = classifier.predict(X_test)\n",
        "prediction_train = classifier.predict(X_train)\n",
        "\n",
        "print(f\"Training Set Accuracy : {accuracy_score(y_train, prediction_train) * 100} %\\n\")\n",
        "print(f\"Test Set Accuracy : {accuracy_score(y_test, prediction) * 100} % \\n\\n\")\n",
        "print(f\"Classifier Report : \\n\\n {classification_report(y_test, prediction)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy : 99.37446074201898 %\n",
            "\n",
            "Test Set Accuracy : 97.84482758620689 % \n",
            "\n",
            "\n",
            "Classifier Report : \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98       798\n",
            "           1       0.99      0.94      0.96       362\n",
            "\n",
            "    accuracy                           0.98      1160\n",
            "   macro avg       0.98      0.97      0.97      1160\n",
            "weighted avg       0.98      0.98      0.98      1160\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}